{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Object serialization example\n",
    "\n",
    "import pickle\n",
    "\n",
    "a = {'hello': 'world'}\n",
    "\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jacek/projects/cqa-sem-eval\n"
     ]
    }
   ],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "import gensim\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# import word2vec_model.word2vec_utils as word2vec\n",
    "\n",
    "def load(file_name):\n",
    "    with open(file_name, 'r', encoding=\"utf8\") as myfile:\n",
    "        return BeautifulSoup(myfile.read(), \"xml\")\n",
    "    \n",
    "def remove_subject_from_question(question):\n",
    "    a =  re.sub('.+\\/\\/\\ ', '', question);\n",
    "    if (a != question):\n",
    "        print(question + \" \" + a + \"\\n\")\n",
    "    return a\n",
    "\n",
    "def subtask_B_word2vec_dataset(xml_file):\n",
    "    print(\"Loading word2vec dataset...\")\n",
    "    \n",
    "    dataset = subtask_A_raw_dataset(xml_file)\n",
    "    \n",
    "\n",
    "def label_to_class(label):\n",
    "    if label == \"PerfectMatch\":\n",
    "        return 0\n",
    "    if label == \"Relevant\":\n",
    "        return 1\n",
    "    if label == \"Irrelevant\":\n",
    "        return 2\n",
    "    \n",
    "def label_to_class_2(label):\n",
    "    if label == \"Irrelevant\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def load_file_subject_body_merged(xml_file):\n",
    "    print(\"Loading raw subject - body dataset\")\n",
    "    \n",
    "    soup = load(xml_file)\n",
    "    original_questions = soup.findAll(\"OrgQuestion\")\n",
    "    data_set = []\n",
    "    \n",
    "    for original_question in original_questions:\n",
    "        related_questions = original_question.findAll(\"RelQuestion\")\n",
    "        \n",
    "        for related_question in related_questions:\n",
    "            \n",
    "            original_question_text = original_question.OrgQSubject.text + \". \" + original_question.OrgQBody.text\n",
    "            related_question_text = related_question.RelQSubject.text + \". \" + related_question.RelQBody.text\n",
    "            \n",
    "            data_set_sample = dict([(\"original_question\", remove_subject_from_question(original_question_text)),\n",
    "                                    (\"related_question\", related_question_text),\n",
    "                                    (\"relevance\", label_to_class_2(related_question['RELQ_RELEVANCE2ORGQ']))])\n",
    "            \n",
    "#             print(data_set_sample)\n",
    "            data_set.append(data_set_sample)\n",
    "            \n",
    "    print(\"Loading raw data set [DONE]\")\n",
    "    return data_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw subject - body dataset\n",
      "Loading raw data set [DONE]\n"
     ]
    }
   ],
   "source": [
    "test_file_input = \"data/SemEval2017-task3-English-test-input.xml\"\n",
    "# file = \"data/SemEval2016-Task3-CQA-QL-train-part2.xml\"\n",
    "# asd = load_file_subject_body_merged(test_file_input)\n",
    "# train2_body_pairs = load_file_body(file)\n",
    "# train2_body_pairs = load_file_subject(file)\n",
    "\n",
    "asd = load_test_file_subject_body_merged(test_file_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2_body_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('subtask_B/data/test_original_related_subject_body_merged.json', 'w') as outfile:\n",
    "    json.dump(asd, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generowanie z jsona obiektow X i Y \n",
    "\n",
    "with open('original_related_body_relevance_pairs.json', 'r') as file:\n",
    "    data_set = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GOOGLE_TRAINED_VECTORS = 'cqa-sem-eval/data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "def data_to_word2vec_3_classes():\n",
    "    word2vec_model = load_word2vec_model(GOOGLE_TRAINED_VECTORS)\n",
    "    result = []\n",
    "    for sample in data_set:\n",
    "        org_question_vector = word2vec.sentence_vectors_mean(\n",
    "            word2vec.sentence2vectors(sample[\"original_question\"], word2vec_model, exclude_stopwords=True, to_lower_case=True))\n",
    "        rel_question_vector = word2vec.sentence_vectors_mean(\n",
    "            word2vec.sentence2vectors(sample[\"related_question\"], word2vec_model, to_lower_case=True, exclude_stopwords=True))\n",
    "        if len(org_question_vector) == 0 or len(rel_question_vector) == 0:\n",
    "            continue\n",
    "        transformed_sample = dict([\n",
    "            (\"org_question\", org_question_vector),\n",
    "            (\"rel_question\", rel_question_vector),\n",
    "            (\"relevance\", label_to_class(sample[\"relevance\"]))])\n",
    "        result.append(transformed_sample)\n",
    "    print(\"Loading word2vec data set [DONE]\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def data_to_word2vec_2_classes():\n",
    "    word2vec_model = load_word2vec_model(GOOGLE_TRAINED_VECTORS)\n",
    "    result = []\n",
    "    for sample in data_set:\n",
    "        org_question_vector = word2vec.sentence_vectors_mean(\n",
    "            word2vec.sentence2vectors(sample[\"original_question\"], word2vec_model, exclude_stopwords=True, to_lower_case=True))\n",
    "        rel_question_vector = word2vec.sentence_vectors_mean(\n",
    "            word2vec.sentence2vectors(sample[\"related_question\"], word2vec_model, to_lower_case=True, exclude_stopwords=True))\n",
    "        if len(org_question_vector) == 0 or len(rel_question_vector) == 0:\n",
    "            continue\n",
    "        transformed_sample = dict([\n",
    "            (\"org_question\", org_question_vector),\n",
    "            (\"rel_question\", rel_question_vector),\n",
    "            (\"relevance\", label_to_class_2(sample[\"relevance\"]))])\n",
    "        result.append(transformed_sample)\n",
    "    print(\"Loading word2vec data set [DONE]\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2_vec_first_dataset = data_to_word2vec_3_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sklearn\n",
    "import math\n",
    "import os\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "def load_word2vec_model(model_name):\n",
    "    print(\"Loading word2vec model {}\".format(model_name))\n",
    "    path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    model = None\n",
    "    if model_name == \"Q1_model\" or model_name == \"SemEval2016-Task3-CQA-QL-dev_model\":\n",
    "        model = gensim.models.Word2Vec.load(path + \"/\" + model_name)\n",
    "    elif model_name == \"cqa-sem-eval/data/GoogleNews-vectors-negative300.bin\":\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(path + \"/\" + model_name,\n",
    "                                                                binary=True)\n",
    "    else:\n",
    "        raise Exception(\"Unknown word2vec model {}\".format(model_name))\n",
    "    print(\"Loading word2vec model {} [DONE]\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_word2vec_model(GOOGLE_TRAINED_VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = data_to_word2vec_2_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sample in input_data:\n",
    "    org_question_vec = sample[\"org_question\"]\n",
    "    rel_question_vec = sample[\"rel_question\"]\n",
    "    INPUT_DIM = len(org_question_vec) + len(rel_question_vec)\n",
    "    concatenated_vec = np.concatenate((org_question_vec, rel_question_vec), axis=0)\n",
    "    X.append(concatenated_vec)\n",
    "    Y.append(sample[\"relevance\"])\n",
    "X = np.reshape(X, (-1, INPUT_DIM))\n",
    "Y = np.reshape(Y, (-1, 1))\n",
    "Y = keras.utils.to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# obj0, obj1, obj2 are created here...\n",
    "\n",
    "# Saving the objects:\n",
    "with open('org_rel_body_2_categories.pkl', 'wb') as f:\n",
    "    pickle.dump([X, Y], f)\n",
    "\n",
    "# # Getting back the objects:\n",
    "# with open('objs.pkl') as f:  # Python 3: open(..., 'rb')\n",
    "#     obj0, obj1, obj2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('org_rel_body_2_categories.pkl', 'rb') as f:\n",
    "    X, Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jacek/projects/Untitled Folder'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
