Multi-Domain Duplicate Detection Subtask (CQADupStack Task)
Task E: Identify duplicate questions in StackExchange.
 
Given:
 
a new question (aka the original question),
a set of 50 candidate questions, rerank the 50 candidate questions according to their relevance with respect to the original question, and truncate the result list in such a way that only "PerfectMatch" questions appear in it. "Related" and "Irrelevant" questions should not be returned in the truncated list. The gold labels are contained in the RELC_RELEVANCE2ORGQ field of the related XML file. We will evaluate both the position of good questions in the rank, and the length of the returned result list based on the number of good questions existed for each original question; thus, this is again a ranking task, but at the same time a result list truncation task. Evaluation: The official scorer will provide a number of evaluation measures to assess the quality of the output of a system (see the tools page), but the official evaluation measure towards which all systems will be evaluated and ranked will be MAP' as adjusted for truncated lists (Liu et al. 2016).
 
Apart from providing test data in the same domains as the development and training data, we will also supply two different test sets from other domains. The best system will therefore be the one that not only performs well in the ranking and result list truncation of the test data in the same domain, but will also perform well in a cross-domain setting.
 
Data: The datasets that are supplied for this subtask will span four different domains. The format will be the same as for the other subtasks, but there will be two extra layers. Each original question has 50 candidate questions, and these related questions each have a number of comments. On top of that, they have a number of answers, and each answer may have comments as well. The difference between answers and comments is that answers should contain a well formed answer to the question, while comments contain things like requests for clarification, remarks, small additions to someone else's answer, etc. Since the content of StackExchange is provided by the community, this distinction does not consistently hold.
 
The data has also been extended with some meta data fields like the tags that are associated with each question, the number of times a question has been viewed, and the score of each question, answer and comment (the number of upvotes it has received from the community, minus the number of downvotes). Separate files with user statistics are provided, containing information like user reputation, user badges, etc. Participants are allowed to use any of the information provided when calculating their rankings and truncations.
 
The relevance labels in the development and training data provided come directly from the users of the StackExchange sites. Users of the forums can vote for questions to be closed as a duplicate of another one. These are the questions labeled as 'PerfectMatch'. The questions labeled as 'Related' are question that are not duplicates, but that are somehow similar to the original question, also as judged by the StackExchange community. It is possible that some duplicate labels are missing, due to the voluntary nature of the duplicate labeling on StackExchange. The development and training data should therefore be considered a silver standard. For the test data we will conduct exhaustive annotations, to validate whether each candidate question is a duplicate of the new question.