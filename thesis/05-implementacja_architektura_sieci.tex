\chapter{Implementacja}

\section{Wykorzystywane modele}
Ze względu na podobieństwa między zadaniami, pewne fragmenty implementowanych modeli powtarzają się. W tym podrozdziale opisano rodzaje sieci neuronowych, które były w projekcie wykorzystywane najczęściej. Są to rozwiązania często realizowane w dziedzinie przetwarzania języka naturalnego.

\subsection{Prosta sieć neuronowa}
Podstawowa architektura sieci neuronowej opiera się na kolejno połączonych ze sobą warstwach neuronów. Warstwy są łączone w sposób szeregowy, najczęściej w oparciu o zasadę ,,każdy z każdym''. Oznacza to, że wyjścia danej warstwy trafiają dalej do wejść każdego z neuronów warstwy następnej. Za wyjątek można uznać pierwszą i ostatnią warstwę sieci. Te warstwy, nazywane wejściową i wyjściową, mają za zadanie doprowadzić dane wejściowe do sieci i wyprowadzić dane wynikowe jako wyjście. Taki model sieci jest wszechstronny i nieskomplikowany, stanowi dobry punkt wyjścia dla rozwiązań z dziedziny uczenia maszynowego. 

\subsection{Sieci rekurencyjne}
Bardziej złożony model, odpowiedni dla danych sekwencyjnych, stanowi sieć rekurencyjna. Tego typu model posiada swego rodzaju pętle zwrotne. Są to połączenia, które doprowadzają do sieci jej własne wyjście. W praktyce stosuje się najczęściej pojedyncze warstwy rekurencyjne. Pętle w sieci rekurencyjnej pozwalają uzyskać pewną właściwość sieci, którą można by nazwać ,,pamięcią''. Oznacza to, że wyniki przeszłych obliczeń wpływają na obliczenia aktualne. Łatwo zauważyć, że takie działanie sieci imituje ludzki sposób analizy sekwencyjnych danych, np. filmu lub tekstu. Ze względu na tę właściwość, sieci (warstwy) rekurencyjne są powszechnie używane w przetwarzaniu języka naturalnego.

\subsection{Sieci syjamskie} \label{arch:malstm}
Przykładem jednej z nowszych architektur wykorzystywanych w przetwarzaniu języka jest sieć syjamska \cite{malstm:paper}. Podstawową cechą takiej sieci jest wprowadzenie dwóch identycznych podsieci, które posiadają osobne wejścia i wyjścia. Taki podział ścieżek przetwarzania pozwala na uzyskanie mechanizmu porównywania danych wprowadzanych na wejścia podsieci. W modelu ogólnym, obie podsieci, choć zbudowane z tych samych elementów, nie mają wspólnych wag i stanowią osobne byty. W trakcie uczenia każda podsieć może przez to nabrać nieco innych właściwości. Do pewnych celów wykorzystuje się jednak sieci syjamskie, w których wagi lub całe warstwy są współdzielone przez podsieci. Model ogólny może być odpowiedni, kiedy pary danych wprowadzane na wejście sieci syjamskiej różnią się pod pewnymi względami. Przykładem z dziedziny przetwarzania języka naturalnego może być porównywanie treści zapytania, podanej przez użytkownika w wyszukiwarce internetowej, z wybraną odpowiedzią. Zapytanie to zazwyczaj kilka słów, które niekoniecznie tworzą zdanie. Odpowiedź wybrana jako stosowna może być jednak dłuższym i bardziej złożonym tekstem. W takim przypadku ważne jest umożliwienie każdej podsieci nabycie własnych wag w procesie uczenia. W innym przypadku, na przykład porównując pary komentarzy z forum internetowego, korzystniejsze jest stworzenie współdzielonych przez podsieci warstw. Pozwala to przyspieszyć proces uczenia bez obniżania skuteczności modelu.